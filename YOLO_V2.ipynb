{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLO-V2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9nG5KAuIFvr"
      },
      "source": [
        "**IMPORTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2LSlQZcdAqI"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import cv2\n",
        "\n",
        "#for loading weights\n",
        "import struct\n",
        "from collections import defaultdict\n",
        "\n",
        "#utils functions\n",
        "import random \n",
        "import colorsys\n",
        "import scipy\n",
        "import imghdr\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn #building block to create and train NN\n",
        "import torch.nn.functional as F #build network layers\n",
        "import torch.optim as optim #optimized gradient descent\n",
        "\n",
        "import torchvision.transforms as transforms #transforms images to tensors\n",
        "import torchvision.models as models #pre-trained models\n",
        "import torchvision.ops as ops #operators specific for computer vision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F34ck_R4dHcc",
        "outputId": "144ebcff-84c6-4c60-af9a-28d1832bacde"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QsS8KSsdTTo"
      },
      "source": [
        "**YOLO FILTER BOXES (BASED ON PROBABILITY THRESHOLD)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLll06sPdJeH"
      },
      "source": [
        "def yolo_filter_boxes(box_confidence, boxes, box_class_probs,  threshold = 0.6):\n",
        "    \n",
        "    box_scores = box_confidence * box_class_probs\n",
        "\n",
        "    box_classes = torch.argmax(box_scores,axis=-1)\n",
        "    box_class_scores = torch.max(box_scores,axis=-1,keepdim=False)[0].squeeze()\n",
        "    \n",
        "    filtering_mask = (box_class_scores >= threshold)\n",
        "\n",
        "    scores = torch.masked_select(box_class_scores, filtering_mask)\n",
        "    boxes = torch.masked_select(boxes, filtering_mask.unsqueeze(dim=-1)).reshape(-1,4)\n",
        "    classes = torch.masked_select(box_classes, filtering_mask)\n",
        "\n",
        "    return scores, boxes, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLKzPWa3MeT-",
        "outputId": "81500dd1-34ab-43c9-c2bb-1f9014a4a184"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "box_confidence = torch.empty(19,19,5,1).normal_(mean=1,std=4)\n",
        "boxes = torch.empty(19,19,5,4).normal_(mean=1,std=4)\n",
        "box_class_probs = torch.empty(19,19,5,80).normal_(mean=1,std=4)\n",
        "\n",
        "scores, boxes, classes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = 0.5)\n",
        "\n",
        "print(\"scores[2] = \" + str(scores[2]))\n",
        "print(\"boxes[2] = \" + str(boxes[2]))\n",
        "print(\"classes[2] = \" + str(classes[2]))\n",
        "print(\"scores.shape = \" + str(scores.shape))\n",
        "print(\"boxes.shape = \" + str(boxes.shape))\n",
        "print(\"classes.shape = \" + str(classes.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scores[2] = tensor(22.5610)\n",
            "boxes[2] = tensor([ 5.3702, -1.3419,  7.4093,  4.6440])\n",
            "classes[2] = tensor(71)\n",
            "scores.shape = torch.Size([1794])\n",
            "boxes.shape = torch.Size([1794, 4])\n",
            "classes.shape = torch.Size([1794])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVDIpRp5dpOu"
      },
      "source": [
        "**INTERSECTION OVER UNION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DnShdtrdnd2"
      },
      "source": [
        "def iou(box1,box2):\n",
        "    (box1_x1, box1_y1, box1_x2, box1_y2) = box1\n",
        "    (box2_x1, box2_y1, box2_x2, box2_y2) = box2\n",
        "\n",
        "    xi1 = max(box1_x1,box2_x1)\n",
        "    yi1 = max(box1_y1,box2_y1)\n",
        "    xi2 = min(box1_x2,box2_x2)\n",
        "    yi2 = min(box1_y2,box2_y2)\n",
        "\n",
        "    intersection_width = xi2 - xi1\n",
        "    intersection_height = yi2 - yi1\n",
        "    intersection_area = max(intersection_width,0) * max(intersection_height,0)\n",
        "\n",
        "    box1_area = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)\n",
        "    box2_area = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)\n",
        "    union_area = box1_area + box2_area - intersection_area\n",
        "\n",
        "    iou = intersection_area / union_area\n",
        "    return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkoyQ9_qduOE",
        "outputId": "525c8008-5048-4c79-8af9-2874bdde8d41"
      },
      "source": [
        "## Test case 1: boxes intersect\n",
        "box1 = (2, 1, 4, 3)\n",
        "box2 = (1, 2, 3, 4) \n",
        "print(\"iou for intersecting boxes = \" + str(iou(box1, box2)))\n",
        "\n",
        "## Test case 2: boxes do not intersect\n",
        "box1 = (1,2,3,4)\n",
        "box2 = (5,6,7,8)\n",
        "print(\"iou for non-intersecting boxes = \" + str(iou(box1,box2)))\n",
        "\n",
        "## Test case 3: boxes intersect at vertices only\n",
        "box1 = (1,1,2,2)\n",
        "box2 = (2,2,3,3)\n",
        "print(\"iou for boxes that only touch at vertices = \" + str(iou(box1,box2)))\n",
        "\n",
        "## Test case 4: boxes intersect at edge only\n",
        "box1 = (1,1,3,3)\n",
        "box2 = (2,3,3,4)\n",
        "print(\"iou for boxes that only touch at edges = \" + str(iou(box1,box2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iou for intersecting boxes = 0.14285714285714285\n",
            "iou for non-intersecting boxes = 0.0\n",
            "iou for boxes that only touch at vertices = 0.0\n",
            "iou for boxes that only touch at edges = 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xYaMuNmdyOO"
      },
      "source": [
        "**CONVERT [MIDPOINT, HEIGHT, WIDTH] TO [TOP LEFT, BOTTOM RIGHT]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuhOPAEUdv1u"
      },
      "source": [
        "def convert_coordinates(x):\n",
        "    #bx, by, bw, bh >>>> xmin, ymin, xmax, ymax\n",
        "    y = x.new(x.shape)\n",
        "    y[..., 0] = x[..., 0] - x[..., 2] / 2\n",
        "    y[..., 1] = x[..., 1] - x[..., 3] / 2\n",
        "    y[..., 2] = x[..., 0] + x[..., 2] / 2\n",
        "    y[..., 3] = x[..., 1] + x[..., 3] / 2\n",
        "    \n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5U5wFw7O9np",
        "outputId": "a64b4672-087f-41f6-e750-e52ac40d18de"
      },
      "source": [
        "convert_coordinates(torch.tensor([0,0,3,5],dtype=torch.float32))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.5000, -2.5000,  1.5000,  2.5000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMEgivdad2tv"
      },
      "source": [
        "**NON MAX SUPPRESSION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EByExS4Wd0ZW"
      },
      "source": [
        "def yolo_non_max_suppression(boxes, classes, scores, max_boxes = 10, iou_threshold = 0.2):\n",
        "\n",
        "    nms_indices = ops.nms(boxes, scores, iou_threshold)\n",
        "\n",
        "    scores = torch.gather(scores, 0, nms_indices)\n",
        "    classes = torch.gather(classes, 0, nms_indices)\n",
        "    boxes = boxes[nms_indices[:]]\n",
        "\n",
        "    return scores, boxes, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5utvwWFd6zA",
        "outputId": "f5a95893-ee38-4952-fba8-e0b2f4cec996"
      },
      "source": [
        "scores, boxes, classes = yolo_non_max_suppression(boxes, classes, scores)\n",
        "\n",
        "print(scores.shape)\n",
        "print(boxes.shape)\n",
        "print(classes.shape)\n",
        "\n",
        "print(\"scores[2] = \" + str(scores[2]))\n",
        "print(\"boxes[2] = \" + str(boxes[2]))\n",
        "print(\"classes[2] = \" + str(classes[2]))\n",
        "print(\"scores.shape = \" + str(scores.shape))\n",
        "print(\"boxes.shape = \" + str(boxes.shape))\n",
        "print(\"classes.shape = \" + str(classes.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1470])\n",
            "torch.Size([1470, 4])\n",
            "torch.Size([1470])\n",
            "scores[2] = tensor(150.3171)\n",
            "boxes[2] = tensor([11.5748, -0.2527,  4.5467,  0.7380])\n",
            "classes[2] = tensor(10)\n",
            "scores.shape = torch.Size([1470])\n",
            "boxes.shape = torch.Size([1470, 4])\n",
            "classes.shape = torch.Size([1470])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26J8-CF_eBb9"
      },
      "source": [
        "**WRAPPING UP THE FILTERING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6aPbi8bd_Bn"
      },
      "source": [
        "def yolo_eval(yolo_outputs, image_shape = (720., 1280.), max_boxes=10, score_threshold=.6, iou_threshold=.5):\n",
        "    \n",
        "    #Retrieve outputs of the yolo model\n",
        "    box_confidence, box_xywh, box_class_probs = yolo_outputs\n",
        "    #Convert bx,by,bh,bw to x1,y1,x2,y2\n",
        "    boxes = convert_coordinates(box_xywh)\n",
        "    #Filter using score_threshold\n",
        "    scores, boxes, classes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, score_threshold)\n",
        "    #Rescale boxes to original image shape\n",
        "    #boxes = scale_boxes(boxes, image_shape)\n",
        "    #Perform non max suppression given iou threshold\n",
        "    scores, boxes, classes = yolo_non_max_suppression(boxes, classes, scores, iou_threshold)\n",
        "    \n",
        "    return scores, boxes, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3exy_UveELl",
        "outputId": "4252988e-9ae5-436a-c7b9-65e891c8472b"
      },
      "source": [
        "yolo_outputs = (torch.rand(1,19,19,5,1),\n",
        "                torch.rand(1,19,19,5,4),\n",
        "                torch.rand(1,19,19,5,80))\n",
        "                    \n",
        "scores, boxes, classes = yolo_eval(yolo_outputs)\n",
        "\n",
        "print(\"scores[2] = \" + str(scores[2]))\n",
        "print(\"boxes[2] = \" + str(boxes[2]))\n",
        "print(\"classes[2] = \" + str(classes[2]))\n",
        "print(\"scores.shape = \" + str(scores.shape))\n",
        "print(\"boxes.shape = \" + str(boxes.shape))\n",
        "print(\"classes.shape = \" + str(classes.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scores[2] = tensor(0.9936)\n",
            "boxes[2] = tensor([-0.1124,  0.4062,  0.6784,  0.6014])\n",
            "classes[2] = tensor(1)\n",
            "scores.shape = torch.Size([113])\n",
            "boxes.shape = torch.Size([113, 4])\n",
            "classes.shape = torch.Size([113])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dznqXRzYeIfp"
      },
      "source": [
        "**Summary for YOLO**:\n",
        "- Input image (608, 608, 3)\n",
        "- The input image goes through a CNN, resulting in a (19,19,5,85) dimensional output. \n",
        "- After flattening the last two dimensions, the output is a volume of shape (19, 19, 425):\n",
        "    - Each cell in a 19x19 grid over the input image gives 425 numbers. \n",
        "    - 425 = 5 x 85 because each cell contains predictions for 5 boxes, corresponding to 5 anchor boxes, as seen in lecture. \n",
        "    - 85 = 5 + 80 where 5 is because $(p_c, b_x, b_y, b_h, b_w)$ has 5 numbers, and 80 is the number of classes we'd like to detect\n",
        "- You then select only few boxes based on:\n",
        "    - Score-thresholding: throw away boxes that have detected a class with a score less than the threshold\n",
        "    - Non-max suppression: Compute the Intersection over Union and avoid selecting overlapping boxes\n",
        "- This gives you YOLO's final output. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktL0bHeXfXs4"
      },
      "source": [
        "**DEFINING ANCHORS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY5jt8o_eFtN"
      },
      "source": [
        "def read_anchors(anchors_path):\n",
        "    with open(anchors_path) as f:\n",
        "        anchors = f.readline()\n",
        "        anchors = [float(x) for x in anchors.split(',')]\n",
        "        anchors = np.array(anchors).reshape(-1, 2)\n",
        "    return anchors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJTvIytkfjCm",
        "outputId": "f7f34359-f502-4fd0-c9c4-1f0387c24e27"
      },
      "source": [
        "anchors_path = '/content/gdrive/My Drive/yolo_anchors.txt'\n",
        "anchors = read_anchors(anchors_path)\n",
        "anchors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.57273 , 0.677385],\n",
              "       [1.87446 , 2.06253 ],\n",
              "       [3.33843 , 5.47434 ],\n",
              "       [7.88282 , 3.52778 ],\n",
              "       [9.77052 , 9.16828 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVWaBv43fdQH"
      },
      "source": [
        "**DEFINING CLASSES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9m0Ny-WfcSq"
      },
      "source": [
        "def read_classes(classes_path):\n",
        "    with open(classes_path) as f:\n",
        "        class_names = f.readlines()\n",
        "    class_names = [c.strip() for c in class_names]\n",
        "    return class_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsU3x7WffhLE",
        "outputId": "c5140917-4a31-4901-9e05-07132e94c872"
      },
      "source": [
        "classes_path = '/content/gdrive/My Drive/coco_classes.txt'\n",
        "class_names = read_classes(classes_path)\n",
        "len(class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBtU86ueoaJR"
      },
      "source": [
        "**BUILDING THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybiqwfk0MAlm"
      },
      "source": [
        "def space_to_depth(x, block_size):\n",
        "    n, c, h, w = x.size()\n",
        "    unfolded_x = torch.nn.functional.unfold(x, block_size, stride=block_size)\n",
        "    return unfolded_x.view(n, c * block_size ** 2, h // block_size, w // block_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBUiFJ1juAnN"
      },
      "source": [
        "class Yolov2(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Yolov2, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.batchnorm4 = nn.BatchNorm2d(64)\n",
        "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchnorm5 = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.conv6 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchnorm6 = nn.BatchNorm2d(256)\n",
        "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.batchnorm7 = nn.BatchNorm2d(128)\n",
        "        self.conv8 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchnorm8 = nn.BatchNorm2d(256)\n",
        "        \n",
        "        self.conv9 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchnorm9 = nn.BatchNorm2d(512)\n",
        "        self.conv10 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.batchnorm10 = nn.BatchNorm2d(256)\n",
        "        self.conv11 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchnorm11 = nn.BatchNorm2d(512)\n",
        "        self.conv12 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.batchnorm12 = nn.BatchNorm2d(256)\n",
        "        self.conv13 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchnorm13 = nn.BatchNorm2d(512)\n",
        "        \n",
        "        self.conv14 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchnorm14 = nn.BatchNorm2d(1024)\n",
        "        self.conv15 = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.batchnorm15 = nn.BatchNorm2d(512)\n",
        "        self.conv16 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchnorm16 = nn.BatchNorm2d(1024)\n",
        "        self.conv17 = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.batchnorm17 = nn.BatchNorm2d(512)\n",
        "        self.conv18 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchnorm18 = nn.BatchNorm2d(1024)\n",
        "\n",
        "        self.conv19 = nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchnorm19 = nn.BatchNorm2d(1024)\n",
        "        self.conv20 = nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchnorm20 = nn.BatchNorm2d(1024)\n",
        "\n",
        "        self.conv21 = nn.Conv2d(in_channels=512, out_channels=64, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.batchnorm21 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv22 = nn.Conv2d(in_channels=1280, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchnorm22 = nn.BatchNorm2d(1024)\n",
        "        \n",
        "        self.conv23 = nn.Conv2d(in_channels=1024, out_channels=425, kernel_size=1, stride=1, padding=0, bias = True)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(F.leaky_relu(self.batchnorm1(self.conv1(x)), negative_slope=0.1), 2, stride=2)\n",
        "        out = F.max_pool2d(F.leaky_relu(self.batchnorm2(self.conv2(out)), negative_slope=0.1), 2, stride=2)\n",
        "        \n",
        "        out = F.leaky_relu(self.batchnorm3(self.conv3(out)), negative_slope=0.1)\n",
        "        out = F.leaky_relu(self.batchnorm4(self.conv4(out)), negative_slope=0.1)\n",
        "        out = F.leaky_relu(self.batchnorm5(self.conv5(out)), negative_slope=0.1)\n",
        "        out = F.max_pool2d(out, 2, stride=2)\n",
        "        \n",
        "        out = F.leaky_relu(self.batchnorm6(self.conv6(out)), negative_slope=0.1)\n",
        "        out = F.leaky_relu(self.batchnorm7(self.conv7(out)), negative_slope=0.1)\n",
        "        out = F.leaky_relu(self.batchnorm8(self.conv8(out)), negative_slope=0.1)\n",
        "        out = F.max_pool2d(out, 2, stride=2)\n",
        "\n",
        "        out = F.leaky_relu(self.batchnorm9(self.conv9(out)), negative_slope=0.1)\n",
        "        out = F.leaky_relu(self.batchnorm10(self.conv10(out)), negative_slope=0.1)\n",
        "        out = F.leaky_relu(self.batchnorm11(self.conv11(out)), negative_slope=0.1)\n",
        "        out = F.leaky_relu(self.batchnorm12(self.conv12(out)), negative_slope=0.1)\n",
        "        out = F.leaky_relu(self.batchnorm13(self.conv13(out)), negative_slope=0.1)\n",
        "        \n",
        "        passthrough = F.leaky_relu(self.batchnorm21(self.conv21(out)), negative_slope=0.1)\n",
        "        passthrough = space_to_depth(passthrough,2)\n",
        "        \n",
        "        out = F.max_pool2d(out, 2, stride=2)\n",
        "        out = F.leaky_relu(self.batchnorm14(self.conv14(out)), negative_slope=0.1)\n",
        "        out = F.leaky_relu(self.batchnorm15(self.conv15(out)), negative_slope=0.1)\n",
        "        out = F.leaky_relu(self.batchnorm16(self.conv16(out)), negative_slope=0.1)\n",
        "        out = F.leaky_relu(self.batchnorm17(self.conv17(out)), negative_slope=0.1)\n",
        "        out = F.leaky_relu(self.batchnorm18(self.conv18(out)), negative_slope=0.1)\n",
        "\n",
        "        out = F.leaky_relu(self.batchnorm19(self.conv19(out)), negative_slope=0.1)\n",
        "        out = F.leaky_relu(self.batchnorm20(self.conv20(out)), negative_slope=0.1)\n",
        "        \n",
        "        out = torch.cat([passthrough, out], 1)\n",
        "        out = F.leaky_relu(self.batchnorm22(self.conv22(out)), negative_slope=0.1)\n",
        "        out = self.conv23(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2QmonwOWb5s"
      },
      "source": [
        "model = Yolov2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qUlMapMtWYs"
      },
      "source": [
        "#model.conv23.weight == torch.tensor(weights[-435200:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsG22vE1l_RK",
        "outputId": "77ac5945-99e8-40ae-831f-32e70cd2b6ac"
      },
      "source": [
        "model.conv22.weight[31,2,2,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0062, -0.0063,  0.0035], grad_fn=<SliceBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-5K__P1OXz-"
      },
      "source": [
        "**LOADING WEIGHTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECjG9TtWjlmx"
      },
      "source": [
        "weightfile = '/content/gdrive/My Drive/yolov2.weights'\n",
        "fp = open(weightfile, \"rb\")\n",
        "\n",
        "#The first 5 values are header information \n",
        "# 1. Major version number\n",
        "# 2. Minor Version Number\n",
        "# 3. Subversion number \n",
        "# 4. Images seen by the network (during training)\n",
        "header = np.fromfile(fp, dtype = np.int32, count = 4)\n",
        "weights = np.fromfile(fp, dtype = np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrgmp95CxbpV",
        "outputId": "697793a7-c3de-46b4-9cef-247ee1c9934c"
      },
      "source": [
        "num_weights = int(len(weights))\n",
        "params = sum([p.numel() for p in model.parameters()])\n",
        "print('Total params: ',num_weights)\n",
        "print('Trainable params: ',params)\n",
        "print('Non trainable params: ',num_weights-params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total params:  50983561\n",
            "Trainable params:  50962889\n",
            "Non trainable params:  20672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_RJow6oZgrm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab0f185-ef2a-429d-99bc-2f4ab9620bea"
      },
      "source": [
        "group_mapping = defaultdict(lambda: defaultdict())\n",
        "cnt = 0\n",
        "for child in model.children():\n",
        "    if type(child) == nn.Conv2d:\n",
        "        cnt += 1\n",
        "        if cnt == 23:\n",
        "            group_mapping['conv'+str(cnt)] = child\n",
        "        if cnt > 22:\n",
        "            break\n",
        "        group_mapping['conv'+str(cnt)] = child\n",
        "        #group_mapping['bias'+str(cnt)] = child\n",
        "    else:\n",
        "        group_mapping['bias'+str(cnt)] = child\n",
        "\n",
        "for i,j in group_mapping.items():\n",
        "    print(i,j)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1 Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "bias1 BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv2 Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "bias2 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv3 Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "bias3 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv4 Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "bias4 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv5 Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "bias5 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv6 Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "bias6 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv7 Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "bias7 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv8 Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "bias8 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv9 Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "bias9 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv10 Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "bias10 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv11 Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "bias11 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv12 Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "bias12 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv13 Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "bias13 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv14 Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "bias14 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv15 Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "bias15 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv16 Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "bias16 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv17 Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "bias17 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv18 Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "bias18 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv19 Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "bias19 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv20 Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "bias20 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv21 Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "bias21 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv22 Conv2d(1280, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "bias22 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "conv23 Conv2d(1024, 425, kernel_size=(1, 1), stride=(1, 1))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTXY9Gc_hWq1"
      },
      "source": [
        "f = open(weightfile, 'rb')\n",
        "major, minor, revision, seen = struct.unpack('4i', f.read(16))\n",
        "\n",
        "for i in range(1, 24):\n",
        "    if i == 23:\n",
        "            conv_var = group_mapping['conv'+str(i)]\n",
        "            c_out, c_in, f1, f2 = conv_var.weight.size()\n",
        "            cnt = int(c_out * c_in * f1 * f2)\n",
        "            p = struct.unpack('%df' % cnt, f.read(4*cnt))\n",
        "            conv_var.weight.data = torch.from_numpy(np.reshape(p, [c_out, c_in, f1, f2])).float()\n",
        "            for param in conv_var.parameters():\n",
        "                param.requires_grad = False\n",
        "            break\n",
        "\n",
        "    bias_var = group_mapping['bias'+str(i)]\n",
        "    cnt = int(bias_var.bias.size()[0])\n",
        "    bias_var.bias.data = torch.from_numpy(np.array(struct.unpack('%df' % cnt, f.read(4*cnt)))).float()\n",
        "    bias_var.weight.data = torch.from_numpy(np.array(struct.unpack('%df' % cnt, f.read(4*cnt)))).float()\n",
        "    bias_var.running_mean = torch.from_numpy(np.array(struct.unpack('%df' % cnt, f.read(4*cnt)))).float()\n",
        "    bias_var.running_var = torch.from_numpy(np.array(struct.unpack('%df' % cnt, f.read(4*cnt)))).float()\n",
        "    for param in bias_var.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    conv_var = group_mapping['conv'+str(i)]\n",
        "    c_out, c_in, f1, f2 = conv_var.weight.size()\n",
        "    cnt = int(c_out * c_in * f1 * f2)\n",
        "    p = struct.unpack('%df' % cnt, f.read(4*cnt))\n",
        "    conv_var.weight.data = torch.from_numpy(np.reshape(p, [c_out, c_in, f1, f2])).float()\n",
        "    for param in conv_var.parameters():\n",
        "        param.requires_grad = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMDqOYWwY22y",
        "outputId": "bc45c826-1703-4aff-b840-ed57e37da018"
      },
      "source": [
        "model.batchnorm2.bias == torch.tensor(weights[128+864:128+864+64])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDfNHjZeWTfA"
      },
      "source": [
        "**YOLO HEAD**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWa2t8FvUUUS"
      },
      "source": [
        "def yolo_head(feats, anchors, num_classes):\n",
        "    \"\"\"Convert final layer features to bounding box parameters.\n",
        "    Parameters\n",
        "    ----------\n",
        "    feats : tensor\n",
        "        Final convolutional layer features.\n",
        "    anchors : array-like\n",
        "        Anchor box widths and heights.\n",
        "    num_classes : int\n",
        "        Number of target classes.\n",
        "    Returns\n",
        "    -------\n",
        "    box_xy : tensor\n",
        "        x, y box predictions adjusted by spatial location in conv layer.\n",
        "    box_wh : tensor\n",
        "        w, h box predictions adjusted by anchors and conv spatial resolution.\n",
        "    box_conf : tensor\n",
        "        Probability estimate for whether each box contains any object.\n",
        "    box_class_pred : tensor\n",
        "        Probability distribution estimate for each box over class labels.\n",
        "    \"\"\"\n",
        "    num_anchors = len(anchors)\n",
        "    # Reshape to batch, height, width, num_anchors, box_params.\n",
        "    anchors_tensor = torch.reshape(torch.tensor(anchors), (1, 1, 1, num_anchors, 2))\n",
        "\n",
        "    # Static implementation for fixed models.\n",
        "    # TODO: Remove or add option for static implementation.\n",
        "    # _, conv_height, conv_width, _ = K.int_shape(feats)\n",
        "    # conv_dims = K.variable([conv_width, conv_height])\n",
        "\n",
        "    # Dynamic implementation of conv dims for fully convolutional model.\n",
        "    conv_dims = feats.shape[1:3]  # assuming channels last\n",
        "    # In YOLO the height index is the inner most iteration.\n",
        "    conv_height_index = torch.arange(0, end=conv_dims[0])\n",
        "    conv_width_index = torch.arange(0, end=conv_dims[1])\n",
        "    conv_height_index = conv_height_index.repeat(conv_dims[1])\n",
        "\n",
        "    # TODO: Repeat_elements and tf.split doesn't support dynamic splits.\n",
        "    # conv_width_index = K.repeat_elements(conv_width_index, conv_dims[1], axis=0)\n",
        "    conv_width_index = (torch.unsqueeze(conv_width_index, 0)).repeat(conv_dims[0], 1)\n",
        "\n",
        "    conv_width_index = torch.flatten(torch.transpose(conv_width_index,0,1))\n",
        "    conv_index = torch.transpose(torch.stack([conv_height_index, conv_width_index]),0,1)\n",
        "    conv_index = torch.reshape(conv_index, (1, conv_dims[0], conv_dims[1], 1, 2))\n",
        "    conv_index.type(feats.dtype)\n",
        "\n",
        "    feats = torch.reshape(\n",
        "        feats, [-1, conv_dims[0], conv_dims[1], num_anchors, 80 + 5])\n",
        "    conv_dims = torch.reshape(torch.tensor(conv_dims), (1, 1, 1, 1, 2))\n",
        "    conv_dims.type(feats.dtype)\n",
        "\n",
        "    # Static generation of conv_index:\n",
        "    # conv_index = np.array([_ for _ in np.ndindex(conv_width, conv_height)])\n",
        "    # conv_index = conv_index[:, [1, 0]]  # swap columns for YOLO ordering.\n",
        "    # conv_index = K.variable(\n",
        "    #     conv_index.reshape(1, conv_height, conv_width, 1, 2))\n",
        "    # feats = Reshape(\n",
        "    #     (conv_dims[0], conv_dims[1], num_anchors, num_classes + 5))(feats)\n",
        "\n",
        "    box_xy = torch.sigmoid(feats[..., :2])\n",
        "    box_wh = torch.exp(feats[..., 2:4])\n",
        "    box_confidence = torch.sigmoid(feats[..., 4:5])\n",
        "    box_class_probs = torch.softmax(feats[..., 5:], 0, dtype=feats.dtype)\n",
        "\n",
        "    # Adjust preditions to each spatial grid point and anchor size.\n",
        "    # Note: YOLO iterates over height index before width index.\n",
        "    box_xy = ((box_xy + conv_index) / conv_dims).type(feats.dtype)\n",
        "    box_wh = (box_wh * anchors_tensor / conv_dims).type(feats.dtype)\n",
        "\n",
        "    box = torch.cat((box_xy,box_wh),-1)\n",
        "\n",
        "    return box, box_confidence, box_class_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0M8wwHnYVZ4",
        "outputId": "68a9f874-7237-44a2-ea79-599d5ac1d397"
      },
      "source": [
        "t = torch.rand(1,19,19,425)\n",
        "box, box_confidence, box_class_probs = yolo_head(t, anchors, len(classes))\n",
        "\n",
        "print(box_confidence.shape)\n",
        "print(box.shape)\n",
        "print(box_class_probs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 19, 19, 5, 1])\n",
            "torch.Size([1, 19, 19, 5, 4])\n",
            "torch.Size([1, 19, 19, 5, 80])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "140EBYc27qNB"
      },
      "source": [
        "**LOADING IMAGE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AFK1Hm7kbWX"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((608,608)),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "def image_loader(img_dir): # PIL image to 4d tensor (B,C,H,W)\n",
        "    image = Image.open(img_dir)\n",
        "    image = transform(image).unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "img_dir = '/content/gdrive/My Drive/800.jpeg'\n",
        "model_inputs = image_loader(img_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDYYNSlhkbcE",
        "outputId": "ef17ea9c-6c3b-4b13-dcee-ffddf1ae8587"
      },
      "source": [
        "#yolo model input image\n",
        "model_inputs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 608, 608])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AI5W5thq3WZ"
      },
      "source": [
        "**INPUT CYCLE SUMMARY**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zLG4evwU41Y"
      },
      "source": [
        "1. <font color='purple'> yolo_model.input </font> is given to `yolo_model`. The model is used to compute the output <font color='purple'> yolo_model.output </font>\n",
        "2. <font color='purple'> yolo_model.output </font> is processed by `yolo_head`. It gives you <font color='purple'> yolo_outputs </font>\n",
        "3. <font color='purple'> yolo_outputs </font> goes through a filtering function, `yolo_eval`. It outputs your predictions: <font color='purple'> scores, boxes, classes </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHyjcMRoHRbB"
      },
      "source": [
        "**YOLO BODY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovAuolUeAgFO"
      },
      "source": [
        "def yolo_body(model, img_dir):\n",
        "    \n",
        "    #retrieve image from img_dir and convert to tensor\n",
        "    model_inputs = image_loader(img_dir)\n",
        "    #forward pass through the model\n",
        "    model_outputs = model.forward(model_inputs)\n",
        "    #reshape the model_outputs from b,c,h,w to b,h,w,c\n",
        "    model_outputs = model_outputs.permute(0,3,2,1)\n",
        "    #break down the model_outputs to box_confidence (pc), boxes (bx,by,bw,bh), box_class_probs (c1,c2..,c80)\n",
        "    boxes, box_confidence, box_class_probs = yolo_head(model_outputs, anchors, len(class_names))\n",
        "    #combine the box_confidence, boxes, box_class_probs to tuple yolo_outputs\n",
        "    yolo_outputs = (box_confidence, boxes, box_class_probs)\n",
        "    #perform boxes selection using threshold and non max suppression\n",
        "    yolo_scores, yolo_boxes, yolo_classes = yolo_eval(yolo_outputs)\n",
        "    #combine yolo_scores, yolo_boxes, yolo_classes to tuple predictions\n",
        "    predictions = (yolo_scores,yolo_boxes,yolo_classes)\n",
        "    #pass the predictions to draw the new image\n",
        "    out_scores, out_boxes, out_classes = predict(predictions, img_dir)\n",
        "    \n",
        "    return out_scores, out_boxes, out_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD6iKI1KXX_0"
      },
      "source": [
        "#yolo_body(model,img_dir)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ehGpYxGXRMl"
      },
      "source": [
        "**UTILS FUNCTIONS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhtMZCv_pv7H"
      },
      "source": [
        "**GENERATE COLORS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fHYxfDJRB3R"
      },
      "source": [
        "def generate_colors(class_names):\n",
        "    hsv_tuples = [(x / len(class_names), 1., 1.) for x in range(len(class_names))]\n",
        "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
        "    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
        "    random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
        "    random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.\n",
        "    random.seed(None)  # Reset seed to default.\n",
        "    return colors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkM9DUKxqK4q"
      },
      "source": [
        "**PRE PROCESS IMAGE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLSOn0BoRBlw"
      },
      "source": [
        "def preprocess_image(img_path, model_image_size):\n",
        "    image_type = imghdr.what(img_path)\n",
        "    image = Image.open(img_path)\n",
        "    resized_image = image.resize(tuple(reversed(model_image_size)), Image.BICUBIC)\n",
        "    image_data = np.array(resized_image, dtype='float32')\n",
        "    image_data /= 255.\n",
        "    image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
        "    return image, image_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEMZEO5_qPMz"
      },
      "source": [
        "**SCALE BOXES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2kEjBBOqOmi"
      },
      "source": [
        "def scale_boxes(boxes, image_shape):\n",
        "    \"\"\" Scales the predicted boxes in order to be drawable on the image\"\"\"\n",
        "    height = image_shape[0]\n",
        "    width = image_shape[1]\n",
        "    image_dims = K.stack([height, width, height, width])\n",
        "    image_dims = K.reshape(image_dims, [1, 4])\n",
        "    boxes = boxes * image_dims\n",
        "    return boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYmMO6Iapz-v"
      },
      "source": [
        "**DRAW BOXES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY9VIVLNRB0J"
      },
      "source": [
        "def draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors):\n",
        "    \n",
        "    #font = ImageFont.truetype(font='font/FiraMono-Medium.otf',size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
        "    thickness = (image.size[0] + image.size[1]) // 300\n",
        "\n",
        "    for i, c in reversed(list(enumerate(out_classes))):\n",
        "        predicted_class = class_names[c]\n",
        "        box = out_boxes[i]\n",
        "        score = out_scores[i]\n",
        "\n",
        "        label = '{} {:.2f}'.format(predicted_class, score)\n",
        "\n",
        "        draw = ImageDraw.Draw(image)\n",
        "        label_size = draw.textsize(label)\n",
        "\n",
        "        top, left, bottom, right = box\n",
        "        top = max(0, np.floor(top + 0.5).astype('int32'))\n",
        "        left = max(0, np.floor(left + 0.5).astype('int32'))\n",
        "        bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
        "        right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
        "        print(label, (left, top), (right, bottom))\n",
        "\n",
        "        if top - label_size[1] >= 0:\n",
        "            text_origin = np.array([left, top - label_size[1]])\n",
        "        else:\n",
        "            text_origin = np.array([left, top + 1])\n",
        "\n",
        "        # My kingdom for a good redistributable image drawing library.\n",
        "        for i in range(thickness):\n",
        "            draw.rectangle([left + i, top + i, right - i, bottom - i], outline=colors[c])\n",
        "        draw.rectangle([tuple(text_origin), tuple(text_origin + label_size)], fill=colors[c])\n",
        "        #draw.text(text_origin, label, fill=(0, 0, 0))\n",
        "        del draw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQd6qdxEUgtx"
      },
      "source": [
        "**RUN THE GRAPH ON AN IMAGE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp9HyhEJoKbL"
      },
      "source": [
        "def predict(predictions, image_file):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    image_file -- name of an image stored in the \"images\" folder.\n",
        "    \n",
        "    Returns:\n",
        "    out_scores -- tensor of shape (None, ), scores of the predicted boxes\n",
        "    out_boxes -- tensor of shape (None, 4), coordinates of the predicted boxes\n",
        "    out_classes -- tensor of shape (None, ), class index of the predicted boxes\n",
        "    \n",
        "    Note: \"None\" actually represents the number of predicted boxes, it varies between 0 and max_boxes. \n",
        "    \"\"\"\n",
        "\n",
        "    # Preprocess your image\n",
        "    image, image_data = preprocess_image(image_file, model_image_size = (608, 608))\n",
        "\n",
        "    out_scores, out_boxes, out_classes = predictions\n",
        "    out_scores = out_scores.detach().numpy()\n",
        "    out_boxes = out_boxes.detach().numpy()\n",
        "    out_classes = out_classes.detach().numpy()\n",
        "\n",
        "    # Print predictions info\n",
        "    print('Found {} boxes for input image'.format(len(out_boxes)))\n",
        "    # Generate colors for drawing bounding boxes.\n",
        "    colors = generate_colors(class_names)\n",
        "    # Draw bounding boxes on the image file\n",
        "    draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors)\n",
        "    # Display the results in the notebook\n",
        "    plt.imshow(image)\n",
        "    \n",
        "    return out_scores, out_boxes, out_classes"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}